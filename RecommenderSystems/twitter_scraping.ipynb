{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6431881",
   "metadata": {},
   "source": [
    "## Twitter Scraping For Twitch Streamer Recommender System\n",
    "\n",
    "#### By: Aurelio Barrios\n",
    "\n",
    "### What is Shown In This Notebook\n",
    "\n",
    "This notebook is used alongside the `recommenderSystem.ipynb` file to build a recommender system from scratch. What you will find in this notebook is the pseudocode for how the data was scraped off of Twitter using the Tweepy Twitter API. Since the credentials necessary to run this API are personal they wont be included and therefore this notebook will not be displayed with cells that are ran. \n",
    "\n",
    "### Imports\n",
    "\n",
    "First, necessary imports must be loaded in such as the `tweepy` or Twitter API package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee4a73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tweepy\n",
    "import urllib\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ffa2f4",
   "metadata": {},
   "source": [
    "### Credentials\n",
    "\n",
    "Insert credentials needed to load up the Twitter API. Left blank due to sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c483a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#store tweepy api credentials\n",
    "api_key = ''\n",
    "api_secret = ''\n",
    "\n",
    "access_token = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fbf886",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log in to use twitter api\n",
    "auth = tweepy.AppAuthHandler(api_key, api_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a31a70",
   "metadata": {},
   "source": [
    "## Twitter Scraping Using Twitter API \n",
    "\n",
    "We begin with loading a csv file `data/twitter_handles.csv` which contains the Twitch channel name and the respective Twitter account handle for some of the most prominent twitch streamers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b27ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gather twitch and twitter handles\n",
    "twitter_df = pd.read_csv('data/twitter_handles.csv')\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b25d281",
   "metadata": {},
   "source": [
    "### Twitter Scraping: Step One\n",
    "\n",
    "In order to build the recommender system we must first establish where we will get our connections. In this twitter scraper script, the aim is to go through the twitter handles for each of the twitch streamers stored in the `twitter_df` dataframe. For every streamer we will scrape 200 of their followers. These followers will be the basis for establishing the connections needed to build a recommender system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb3d88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through each twitch streamer twitter handle we have\n",
    "for streamer in twitter_df['twitter_handle']:\n",
    "    try:\n",
    "        #get 200 of their followers\n",
    "        followers = tweepy.Cursor(api.get_followers, id=streamer, count=200).items(200)\n",
    "        #store follower information in a list\n",
    "        f_list = [[follower.id, follower.name, follower.screen_name] for follower in followers]\n",
    "    except BaseException as e:\n",
    "        print('Failed on_status:', e)\n",
    "    #build list into dataframe\n",
    "    curr_df = pd.DataFrame(f_list, columns=['id', 'name', 'screen_name'])\n",
    "    #save the dataframe into file\n",
    "    curr_df.to_csv('data/followers/' + streamer + '_200_followers.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f0fddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#used to get the limit status of our API\n",
    "api.rate_limit_status()['resources']['followers']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda1b574",
   "metadata": {},
   "source": [
    "### Twitter Scraping: Step Two\n",
    "\n",
    "Now that we have a set of followers for each streamer we must establish connections between streamers using these followers. The script below loops through all the streamers and the handles of each of their 200 followers that were previously scraped. Due to the rate limits of the API we will only select 20 of the followers rather than using the full 200. For each of the followers we will collect which other streamers they follow in order to build the connections between each streamer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41e997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "count, max_users = 0, 21\n",
    "#loop through each of the streamers\n",
    "for streamer in twitter_df['twitter_handle']:\n",
    "    #read in the file holding the twitter handles of 200 of the streamers followers\n",
    "    streamer_file = 'data/followers/' + streamer + '_200_followers.csv'\n",
    "    curr_df = pd.read_csv(streamer_file).sample(n=200, random_state=0)\n",
    "    #create directory to store scraped data for each streamer\n",
    "    outdir = 'data/streamers/' + streamer    \n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "        curr_index = 1\n",
    "        #loop through 20 of the current streamers followers and scrape users data\n",
    "        for _, row in curr_df.iterrows():\n",
    "            #create file path to store data\n",
    "            outfile = outdir + '/f' + str(curr_index) + '_' + row.screen_name + '.csv'\n",
    "            try:\n",
    "                #scrape a users data and gather 200 of the people that user is following\n",
    "                following = tweepy.Cursor(api.get_friends, id=row.screen_name, count=200).items(200)\n",
    "                #store scraped data into list\n",
    "                follow_list = [[follow.id, follow.name, follow.screen_name]\n",
    "                              for follow in following]\n",
    "                #save current user data into file\n",
    "                curr_follow_df = pd.DataFrame(follow_list, columns=['id', 'name', 'screen_name'])\n",
    "                curr_follow_df.to_csv(outfile, index=False)\n",
    "\n",
    "                curr_index += 1\n",
    "                count += 1\n",
    "            except BaseException as e:\n",
    "                print('Failed on_status:', e)\n",
    "            #only want a max of 20 followers for each streamer\n",
    "            if curr_index == max_users:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fb6bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#used to get rate limit status of API\n",
    "api.rate_limit_status()['resources']['friends']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c3b1de",
   "metadata": {},
   "source": [
    "### Twitter Scraping: Step Three\n",
    "\n",
    "In this final step of twitter scraping we are going to be scraping the profile images of each of the streamers in our dataset. This part of the scraping is not part of the actual recommender system but part of the deployment of the system. This scraped data will be used in the website where the system will be deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d207c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build dataframe to store scraped data.\n",
    "cols = ['twitter_handle', 'profile_img_normal']\n",
    "img_data = pd.DataFrame(columns=cols)\n",
    "#loop through all the streamers of interest\n",
    "for user in list(twitter_df['twitter_handle']):\n",
    "    try:\n",
    "        #for each user we will get the users profile image url\n",
    "        user_obj = api.get_user(screen_name=user)\n",
    "        url = user_obj.profile_image_url\n",
    "    except:\n",
    "        print('Failed with user:', user)\n",
    "        url = ''\n",
    "    #add scraped data to our storage dataset\n",
    "    curr_df = pd.DataFrame([[user, url]], columns=cols)\n",
    "    img_data = img_data.append(curr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daf6dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user to get rate limit status of API used\n",
    "api.rate_limit_status()['resources']['users']['/users/:id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d767fe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to get all sizes of the twitter profile image\n",
    "def image_builder(x, replace_with=''):\n",
    "    if replace_with == '_original':\n",
    "        return x.replace('_normal', '')\n",
    "    return x.replace('_normal', replace_with)\n",
    "\n",
    "#get all sizes of the twitter profile images\n",
    "for size_tag in ['_bigger', '_mini', '_original']:\n",
    "    img_data['profile_img' + size_tag] = img_data['profile_img_normal'].apply(lambda x: \n",
    "                                                    image_builder(x, replace_with=size_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daa9ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to return the width and length of each image saved\n",
    "def get_img_dimensions(img_url):\n",
    "    file = urllib.request.urlopen(img_url)\n",
    "    im = Image.open(file)\n",
    "    return im.size\n",
    "\n",
    "img_data = img_data.reset_index(drop=True)\n",
    "#get image dimensions\n",
    "img_data['dimensions'] = img_data['profile_img_original'].apply(get_img_dimensions)\n",
    "#build width and heigh columns from dimensions\n",
    "img_data[['width', 'height']] = pd.DataFrame(img_data['dimensions'].tolist(), \n",
    "                                             index=img_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6023ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save our image data into a csv file\n",
    "img_data.to_csv('data/profile_img.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
